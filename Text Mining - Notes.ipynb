{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c6f4f72",
   "metadata": {},
   "source": [
    "Questo notebook è pensato come introduzione agli strumenti di python per *text mining* e *Natural Language Processing*\n",
    "\n",
    "**Text Mining**: is the process of deriving high quality information from text data. The overall goal is to turn the text data into data for analysis, via application of NLP techniques.\n",
    "\n",
    "**Natural Language processing**:is a subfield of linguistics, computer science and AI concerned with interactions between computers and human language. In particular, NLP is focused on how to program computers to process and analyze large amounts of text data.\n",
    "\n",
    "\n",
    "First of all, let's introduce the most used python library for text mining and text data processing: *nltk*, which stands for Natural Language Toolkit.\n",
    "For more information, read the documentation here https://www.nltk.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51297a64",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "It is the first step in NLP, that consists in breaking strings into tokens which are small structures or units. Tokenizations involves three steps:\n",
    "\n",
    "1. Breaking complex sentence into words\n",
    "2. Understanding the importance of each word w.r.t the sentence\n",
    "3. Produce a structural description on an input sentence\n",
    "\n",
    "\n",
    "##### 1. Breaking complex sentence into words\n",
    "For this we simply use the function *word_tokenize* fromm the nltk library. In the module *nltk.tokenize* there is plenty of other functions you can use for this: more informations at https://www.nltk.org/api/nltk.tokenize.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a83bd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Daniel',\n",
       " 'Pennac',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'my',\n",
       " 'favourite',\n",
       " 'writers',\n",
       " '.',\n",
       " 'His',\n",
       " 'main',\n",
       " 'saga',\n",
       " 'is',\n",
       " 'set',\n",
       " 'in',\n",
       " 'a',\n",
       " 'neighborhood',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Parisian',\n",
       " 'suburbs',\n",
       " ',',\n",
       " 'called',\n",
       " 'Belleville',\n",
       " '.',\n",
       " 'Seven',\n",
       " 'books',\n",
       " 'belongs',\n",
       " 'to',\n",
       " 'the',\n",
       " 'saga',\n",
       " ':',\n",
       " 'the',\n",
       " 'first',\n",
       " 'one',\n",
       " 'was',\n",
       " 'written',\n",
       " 'in',\n",
       " '1991',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'last',\n",
       " 'one',\n",
       " 'in',\n",
       " '2017',\n",
       " '.',\n",
       " 'All',\n",
       " 'the',\n",
       " 'books',\n",
       " 'were',\n",
       " 'critical',\n",
       " 'acclaimed',\n",
       " ',',\n",
       " 'but',\n",
       " 'personally',\n",
       " 'the',\n",
       " 'one',\n",
       " 'I',\n",
       " 'prefer',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " ':',\n",
       " 'the',\n",
       " 'first',\n",
       " 'scene',\n",
       " ',',\n",
       " 'where',\n",
       " 'a',\n",
       " 'sheet',\n",
       " 'of',\n",
       " 'ice',\n",
       " 'shape',\n",
       " 'is',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'the',\n",
       " 'African',\n",
       " 'continent',\n",
       " ',',\n",
       " 'is',\n",
       " 'brilliant',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LIBRARIES \n",
    "import nltk\n",
    "import nltk.corpus # sample text for performing tokenization\n",
    "from nltk.tokenize import word_tokenize # importing word_tokenize from nltk\n",
    "\n",
    "\n",
    "\n",
    "text = \"Daniel Pennac is one of my favourite writers. His main saga is set in a neighborhood in the Parisian suburbs, called Belleville. Seven books belongs to the saga: the first one was written in 1991, and the last one in 2017. All the books were critical acclaimed, but personally the one I prefer is the second: the first scene, where a sheet of ice shape is compared to the African continent, is brilliant.   \" \n",
    "token = word_tokenize(text)\n",
    "token = word_tokenize(text)# Passing the string text into word tokenize for breaking the sentences\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7120a8df",
   "metadata": {},
   "source": [
    "##### 2. Understanding the importance of each word w.r.t the sentence\n",
    "For this a method based on the *FreqDist* function from the *nltk.probability* module, which gives you the frequency of words within a text. More information at https://www.nltk.org/api/nltk.probability.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5621feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 9, 'is': 5, ',': 5, 'one': 4, '.': 4, 'in': 4, 'of': 2, 'saga': 2, 'a': 2, 'books': 2, ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f247920",
   "metadata": {},
   "source": [
    "If you have a large text and you want to find the *n* most common words, you can use the *most_common* attribute. Moreover, remark that statistics like these are more informative afeter stop-world removal and stemming.\n",
    "\n",
    "Remark also that if two words have the same frequency (as the world *one* and the world *in* in the example) the most_common function returns the first world that appearse in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a752e765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 9), ('is', 5), (',', 5), ('one', 4)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=4\n",
    "fdist.most_common(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a47906",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming usually refers to normalizing words into its base form or root form.\n",
    "Below an example of what I mean:\n",
    "    \n",
    "                         | Waited -----> Wait |\n",
    "                         | Waiting ----> Wait |\n",
    "                         | Waits ------> Wait |\n",
    "    \n",
    "    \n",
    "    \n",
    "There are two populars algorithms use for stemming: \n",
    " - *Porter Stemming*: removes common morphological and infalctional endings from words\n",
    " - *Lancaster Stemming*: a more agressive steming algorithm\n",
    " \n",
    "For a more complete overview on stemming with python, let's refer to https://towardsdatascience.com/stemming-corpus-with-nltk-7a6a6d02d3e5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f8f17ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waited:wait\n",
      "waiting:wait\n",
      "waits:wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "\n",
    "# Checking for the list of words\n",
    "stm = [\"waited\", \"waiting\", \"waits\"]\n",
    "for word in stm:\n",
    "    print(word+ \":\" +pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9578e130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:giv\n",
      "given:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "stm = [\"giving\", \"given\", \"given\", \"gave\"]\n",
    "for word in stm :\n",
    "    print(word+ \":\" +lst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b436c",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "It is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care,’ whereas stemming would cutoff the ‘ing’ part and convert it into a car.\n",
    "\n",
    "Lemmatization can be implemented in python by using several different algorithms:\n",
    " - Wordnet Lemmatizer\n",
    " - Spacy Lemmatizer\n",
    " - TextBlob \n",
    " - Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58995f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ceb7b",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0ac598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:\n",
      "['daniel', 'pennac', 'is', 'one', 'of', 'my', 'favourite', 'writers', '.', 'his', 'main', 'saga', 'is', 'set', 'in', 'a', 'neighborhood', 'in', 'the', 'parisian', 'suburbs', ',', 'called', 'belleville', '.', 'seven', 'books', 'belongs', 'to', 'the', 'saga', ':', 'the', 'first', 'one', 'was', 'written', 'in', '1991', ',', 'and', 'the', 'last', 'one', 'in', '2017.', 'all', 'the', 'books', 'were', 'critical', 'acclaimed', ',', 'but', 'personally', 'the', 'one', 'i', 'prefer', 'is', 'the', 'second', ':', 'the', 'first', 'scene', ',', 'where', 'a', 'sheet', 'of', 'ice', 'shape', 'is', 'compared', 'to', 'the', 'african', 'continent', ',', 'is', 'brilliant', '.']\n",
      "\n",
      "AFETR STOP-WORD REMOVAL:\n",
      "['daniel', 'pennac', 'one', 'favourite', 'writers', '.', 'main', 'saga', 'set', 'neighborhood', 'parisian', 'suburbs', ',', 'called', 'belleville', '.', 'seven', 'books', 'belongs', 'saga', ':', 'first', 'one', 'written', '1991', ',', 'last', 'one', '2017.', 'books', 'critical', 'acclaimed', ',', 'personally', 'one', 'prefer', 'second', ':', 'first', 'scene', ',', 'sheet', 'ice', 'shape', 'compared', 'african', 'continent', ',', 'brilliant', '.']\n"
     ]
    }
   ],
   "source": [
    "# importing stopwors from nltk library\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "text_old = \"Daniel Pennac is one of my favourite writers. His main saga is set in a neighborhood in the Parisian suburbs, called Belleville. Seven books belongs to the saga: the first one was written in 1991, and the last one in 2017. All the books were critical acclaimed, but personally the one I prefer is the second: the first scene, where a sheet of ice shape is compared to the African continent, is brilliant.   \" \n",
    "a = set(stopwords.words(\"english\"))\n",
    "text1 = word_tokenize(text_old.lower())\n",
    "print(\"ORIGINAL:\")\n",
    "print(text1)\n",
    "stopwords = [x for x in text1 if x not in a]\n",
    "print()\n",
    "print(\"AFETR STOP-WORD REMOVAL:\")\n",
    "print(stopwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
